{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa266a04",
   "metadata": {},
   "source": [
    "# Dendritic Spine Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8542faf5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spine_analysis.spine.grouping import SpineGrouping\n",
    "from spine_analysis.shape_metric.io_metric import SpineMetricDataset\n",
    "from spine_clusterization import SpineClusterizer, DBSCANSpineClusterizer\n",
    "from notebook_widgets import SpineMeshDataset, intersection_ratios_mean_distance, create_dir\n",
    "from spine_segmentation import apply_scale\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "from typing import Optional\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "dataset_path = \"0.025 0.025 0.1 dataset\"\n",
    "scale = (1, 1, 1)\n",
    "show_reduction_method = \"tsne\"\n",
    "    \n",
    "# load meshes and apply scale\n",
    "spine_dataset = SpineMeshDataset().load(dataset_path)\n",
    "spine_dataset.apply_scale(scale)\n",
    "\n",
    "# load merged and reduced manual classification\n",
    "manual_classification = SpineGrouping().load(f\"{dataset_path}/manual_classification/manual_classification_merged_reduced.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ba149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metrics\n",
    "import csv\n",
    "import ctypes as ct\n",
    "\n",
    "import spine_analysis\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(spine_analysis)\n",
    "importlib.reload(spine_analysis.shape_metric.approximation_metric)\n",
    "from spine_analysis.shape_metric.approximation_metric import LightFieldZernikeMomentsSpineMetric\n",
    "\n",
    "csv.field_size_limit(int(ct.c_ulong(-1).value // 2))\n",
    "metrics_path = \"output/clustering_normalized_d=25/metrics.csv\"\n",
    "spine_metrics_lf = SpineMetricDataset().load(metrics_path)#f\"{dataset_path}/metrics.csv\")\n",
    "lf_name = 'LightFieldZernikeMoments'\n",
    "meshes_names = list(spine_dataset.spine_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "distanses = np.identity((len(meshes_names)))\n",
    "distanses *= -1\n",
    "for i, name_1 in enumerate(meshes_names):\n",
    "  for j, name_2 in enumerate(meshes_names):\n",
    "    name_1 = name_1.replace('0.025 0.025 0.1 dataset', '0.025-0.025-0.1-dataset')\n",
    "    name_2 = name_2.replace('0.025 0.025 0.1 dataset', '0.025-0.025-0.1-dataset')\n",
    "    if i == j:\n",
    "      continue\n",
    "    distanses[i,j] = LightFieldZernikeMomentsSpineMetric.distance(spine_metrics_lf.element(name_1, lf_name), spine_metrics_lf.element(name_2, lf_name))\n",
    "print(distanses[distanses >= 0].min(), distanses.max())\n",
    "distanses[distanses <= 0] = 10e15\n",
    "\n",
    "k = meshes_names.index('0.025 0.025 0.1 dataset/1011-1/spine_1.off')\n",
    "l = meshes_names.index('0.025 0.025 0.1 dataset/1003-22/spine_1.off')\n",
    "print(k, l)\n",
    "print(distanses[k,l], distanses[l,k])\n",
    "\n",
    "index = distanses.argmin()\n",
    "i = index // len(meshes_names)\n",
    "j = index % len(meshes_names)\n",
    "print(distanses[i,j], distanses[j,i])\n",
    "print(meshes_names[i], meshes_names[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098864fd",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from spine_analysis.shape_metric.io_metric import SpineMetricDataset\n",
    "from notebook_widgets import SpineMeshDataset, intersection_ratios_mean_distance, create_dir\n",
    "from spine_segmentation import apply_scale\n",
    "from spine_analysis.spine.grouping import SpineGrouping\n",
    "from spine_analysis.clusterization import SpineClusterizer, DBSCANSpineClusterizer\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "dataset_path = \"0.025 0.025 0.1 dataset\"\n",
    "scale = (1, 1, 1)\n",
    "    \n",
    "# load meshes and apply scale\n",
    "spine_dataset = SpineMeshDataset().load(dataset_path)\n",
    "spine_dataset.apply_scale(scale)\n",
    "\n",
    "# load merged and reduced manual classification\n",
    "manual_classification = SpineGrouping().load(f\"{dataset_path}/manual_classification/manual_classification_merged_reduced.json\")\n",
    "manual_classification = manual_classification.get_spines_subset(spine_dataset.spine_names)\n",
    "\n",
    "# load metrics\n",
    "spine_metrics = SpineMetricDataset().load(f\"{dataset_path}/metrics.csv\")\n",
    "spine_metrics = spine_metrics.get_spines_subset(manual_classification.samples)\n",
    "\n",
    "# extract metric subsets OldChordDistribution,OpenAngle,CVD,AverageDistance,LengthVolumeRatio,LengthAreaRatio,JunctionArea,Length,Area,Volume,ConvexHullVolume,ConvexHullRatio\n",
    "classic = spine_metrics.get_metrics_subset(['OpenAngle', 'CVD', 'AverageDistance', 'Length', 'Area', 'Volume', 'ConvexHullVolume', 'ConvexHullRatio'])\n",
    "# extract metric subsets\n",
    "classic = spine_metrics.get_metrics_subset(['OpenAngle', 'CVD', \"JunctionArea\", 'AverageDistance', 'Length', 'Area', 'Volume', 'ConvexHullVolume', 'ConvexHullRatio', \"LengthVolumeRatio\", \"LengthAreaRatio\"])\n",
    "chord = spine_metrics.get_metrics_subset(['OldChordDistribution'])\n",
    "\n",
    "# set score function to mean distance between class over cluster distributions\n",
    "#score_func = lambda clusterizer: intersection_ratios_mean_distance(manual_classification, clusterizer.grouping, False)\n",
    "\n",
    "\n",
    "# prepare folders for export\n",
    "create_dir(f\"{dataset_path}/clustering\")\n",
    "classic_save_path = f\"{dataset_path}/clustering/classic\"\n",
    "create_dir(classic_save_path)\n",
    "chord_save_path = f\"{dataset_path}/clustering/chord/euclidean\"\n",
    "create_dir(f\"{dataset_path}/clustering/chord\")\n",
    "create_dir(f\"{dataset_path}/clustering/chord/euclidean\")\n",
    "chord_js_save_path = f\"{dataset_path}/clustering/chord/jensen-shannon\"\n",
    "create_dir(f\"{dataset_path}/clustering/chord/jensen-shannon\")\n",
    "\n",
    "# elbow method\n",
    "def kmeans_elbow_score(clusterizer: SpineClusterizer) -> float:\n",
    "    # sum of mean distances to cluster center\n",
    "    output = 0\n",
    "    for group in clusterizer.grouping.groups.values():\n",
    "        center = sum(clusterizer.fit_metrics.row_as_array(spine_name) for spine_name in group)\n",
    "        output += sum(np.inner(center - clusterizer.fit_metrics.row_as_array(spine_name),\n",
    "                               center - clusterizer.fit_metrics.row_as_array(spine_name)) for spine_name in group)\n",
    "    return output\n",
    "\n",
    "\n",
    "def dbscan_elbow_score(clusterizer: DBSCANSpineClusterizer) -> float:\n",
    "    # number of points with not enough neighbours close enough to form a cluster\n",
    "    neigh = NearestNeighbors(n_neighbors=clusterizer.min_samples, metric=clusterizer.metric)\n",
    "    data = clusterizer.fit_metrics.as_array()\n",
    "    nbrs = neigh.fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    # get distances to closest k-th neighbour\n",
    "    distances = distances[:, -1]\n",
    "    # sort distances in descending order\n",
    "    distances = -np.sort(-distances, axis=0)\n",
    "    for i in range(len(distances)):\n",
    "        if clusterizer.eps > distances[i]:\n",
    "            return i\n",
    "    return len(distances)\n",
    "\n",
    "def silhouette(clusterizer: SpineClusterizer, metric: Optional[callable] = None) -> float:\n",
    "    datas = []\n",
    "    labels = []\n",
    "    for i, group in enumerate(clusterizer.grouping.groups.values()):\n",
    "        datas.extend(clusterizer.fit_metrics.row_as_array(spine) for spine in group)\n",
    "        labels.extend([i for _ in group])\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    if metric is None:\n",
    "        score = silhouette_score(datas, labels, metric=clusterizer.metric)\n",
    "    else:\n",
    "        score = silhouette_score(np.array([[metric(x1, x2) for x1 in datas] for x2 in datas]), labels, metric=\"precomputed\")\n",
    "    return score\n",
    "\n",
    "def js_distance(x, y) -> float:\n",
    "    return np.sqrt(jensenshannon(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020b362",
   "metadata": {},
   "source": [
    "## k-Means Classic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae2359",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from notebook_widgets import k_means_clustering_experiment_widget\n",
    "\n",
    "# score_func = lambda clusterizer: intersection_ratios_mean_distance(manual_classification, clusterizer.grouping, False)\n",
    "#score_func = silhouette\n",
    "score_func = kmeans_elbow_score\n",
    "\n",
    "dim_reduction = \"\"\n",
    "\n",
    "display(k_means_clustering_experiment_widget(classic, spine_metrics, spine_dataset, score_func,\n",
    "                                             max_num_of_clusters=100, classification=manual_classification,\n",
    "                                             save_folder=classic_save_path, dim_reduction=dim_reduction, show_method=show_reduction_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2273838",
   "metadata": {},
   "source": [
    "## k-Means Chord Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39664cc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from notebook_widgets import k_means_clustering_experiment_widget\n",
    "\n",
    "# score_func = lambda clusterizer: intersection_ratios_mean_distance(manual_classification, clusterizer.grouping, False)\n",
    "score_func = kmeans_elbow_score\n",
    "\n",
    "display(k_means_clustering_experiment_widget(chord, spine_metrics, spine_dataset, score_func,\n",
    "                                             max_num_of_clusters=100, classification=manual_classification,\n",
    "                                             save_folder=chord_save_path, dim_reduction=\"\", show_method=show_reduction_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a461846",
   "metadata": {},
   "source": [
    "## DBSCAN Classic Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3466fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from notebook_widgets import dbscan_clustering_experiment_widget\n",
    "\n",
    "min_eps = 0.2\n",
    "max_eps = 6\n",
    "eps_step = 0.1\n",
    "\n",
    "# score_func = lambda clusterizer: intersection_ratios_mean_distance(manual_classification, clusterizer.grouping, False)\n",
    "score_func = dbscan_elbow_score\n",
    "\n",
    "display(dbscan_clustering_experiment_widget(classic, spine_metrics, spine_dataset, score_func,\n",
    "                                            min_eps=min_eps, max_eps=max_eps, eps_step=eps_step, dim_reduction=\"pca\", show_method=show_reduction_method,\n",
    "                                            classification=manual_classification, save_folder=classic_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a443b94",
   "metadata": {},
   "source": [
    "## DBSCAN Chord Histograms Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec6c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_widgets import dbscan_clustering_experiment_widget\n",
    "\n",
    "min_eps = 0.1\n",
    "max_eps = 10\n",
    "eps_step = 0.1\n",
    "\n",
    "# score_func = lambda clusterizer: intersection_ratios_mean_distance(manual_classification, clusterizer.grouping, False)\n",
    "score_func = dbscan_elbow_score\n",
    "\n",
    "display(dbscan_clustering_experiment_widget(chord, spine_metrics, spine_dataset, score_func,\n",
    "                                            min_eps=min_eps, max_eps=max_eps, eps_step=eps_step, dim_reduction=\"\", show_method=show_reduction_method,\n",
    "                                            classification=manual_classification, save_folder=chord_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea218e3a",
   "metadata": {},
   "source": [
    "## DBSCAN Chord Histograms Jensen â€” Shannon Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0936034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_widgets import dbscan_clustering_experiment_widget\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import numpy as np\n",
    "\n",
    "min_eps = 0.1\n",
    "max_eps = 1\n",
    "eps_step = 0.01\n",
    "use_pca = False\n",
    "\n",
    "# score_func = lambda clusterizer: intersection_ratios_mean_distance(manual_classification, clusterizer.grouping, False)\n",
    "score_func = dbscan_elbow_score\n",
    "\n",
    "def js_distance(x, y) -> float:\n",
    "    return np.sqrt(jensenshannon(x, y))\n",
    "\n",
    "display(dbscan_clustering_experiment_widget(chord, spine_metrics, spine_dataset, score_func, metric=js_distance,\n",
    "                                            min_eps=min_eps, max_eps=max_eps, eps_step=eps_step, use_pca=use_pca,\n",
    "                                            classification=manual_classification, save_folder=chord_js_save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff835c32",
   "metadata": {},
   "source": [
    "## K-Means Spherical Harmonics descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f67345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '0.025-0.025-0.1-dataset'\n",
    "output_dir = \"output/clustering_normalized_shpHarm\"\n",
    "cur_metrics = [\"SphericalGarmonics\"]\n",
    "shpHarm_prefix = f\"sphHarm\"\n",
    "calculate_metrics = False\n",
    "save_metrics = True\n",
    "standardize_metrics = True\n",
    "scale = (1, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806c74c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from spine_analysis.shape_metric.io_metric import SpineMetricDataset\n",
    "from notebook_widgets import k_means_clustering_experiment_widget, create_dir, SpineMeshDataset\n",
    "\n",
    "create_dir(output_dir)\n",
    "spine_dataset = SpineMeshDataset().load(dataset_folder)\n",
    "spine_dataset.apply_scale(scale)\n",
    "\n",
    "if calculate_metrics:\n",
    "    every_spine_metrics = SpineMetricDataset()\n",
    "    every_spine_metrics.calculate_metrics(spine_dataset.spine_meshes, cur_metrics)\n",
    "\n",
    "    if save_metrics:\n",
    "        every_spine_metrics.save(f\"{output_dir}/metrics.csv\")\n",
    "\n",
    "every_spine_metrics = SpineMetricDataset.load(f\"{output_dir}/metrics.csv\")\n",
    "if standardize_metrics:\n",
    "    every_spine_metrics.standardize()\n",
    "\n",
    "sphHarm = every_spine_metrics.get_metrics_subset(cur_metrics)\n",
    "score_func = lambda clusterizer: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f515226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spine_analysis.shape_metric import LightFieldZernikeMomentsSpineMetric\n",
    "from notebook_widgets import kernel_k_means_clustering_experiment_widget\n",
    "from notebook_widgets import SpineMeshDataset\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# load meshes and apply scale\n",
    "spine_dataset = SpineMeshDataset().load(dataset_folder)\n",
    "display(k_means_clustering_experiment_widget(sphHarm, every_spine_metrics, spine_dataset, score_func, metric=distance.cityblock,\n",
    "                                             max_num_of_clusters=5, use_pca=False,\n",
    "                                             filename_prefix=\"sphHarm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082578f4",
   "metadata": {},
   "source": [
    "## Kernel k-Means Zernike Moments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '0.025-0.025-0.1-dataset'\n",
    "output_dir = \"output/clustering_normalized\"\n",
    "cur_metrics = [\"LightFieldZernikeMoments\"]\n",
    "calculate_metrics = False\n",
    "save_metrics = True\n",
    "standardize_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3127a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spine_analysis.mesh.utils import load_spine_meshes\n",
    "from spine_analysis.shape_metric.io_metric import SpineMetricDataset\n",
    "from notebook_widgets import k_means_clustering_experiment_widget, create_dir\n",
    "\n",
    "create_dir(output_dir)\n",
    "spine_meshes = load_spine_meshes(folder_path=dataset_folder)\n",
    "\n",
    "if calculate_metrics:\n",
    "    every_spine_metrics = SpineMetricDataset()\n",
    "    every_spine_metrics.calculate_metrics(spine_meshes, cur_metrics)\n",
    "\n",
    "    if save_metrics:\n",
    "        every_spine_metrics.save(f\"{output_dir}/metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spine_analysis.shape_metric.io_metric import SpineMetricDataset\n",
    "from spine_analysis.clusterization.postprocess import score as score_func\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "every_spine_metrics = SpineMetricDataset.load(f\"{output_dir}/metrics.csv\")\n",
    "if standardize_metrics:\n",
    "    every_spine_metrics.standardize()\n",
    "\n",
    "zernike = every_spine_metrics.get_metrics_subset(['LightFieldZernikeMoments'])\n",
    "zernike_real = every_spine_metrics.clasterization_preprocess(zernike_postprocess='real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceccfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spine_analysis.shape_metric import LightFieldZernikeMomentsSpineMetric\n",
    "from notebook_widgets import kernel_k_means_clustering_experiment_widget\n",
    "from notebook_widgets import SpineMeshDataset\n",
    "\n",
    "# load meshes and apply scale\n",
    "spine_dataset = SpineMeshDataset().load(dataset_folder)\n",
    "display(kernel_k_means_clustering_experiment_widget(zernike, every_spine_metrics, spine_dataset, score_func, metric=LightFieldZernikeMomentsSpineMetric.repr_distance,\n",
    "                                             max_num_of_clusters=4, use_pca=False,\n",
    "                                             filename_prefix=\"zernike\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4305c5",
   "metadata": {},
   "source": [
    "## View clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e99db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_widgets import inspect_saved_groupings_widget\n",
    "\n",
    "display(inspect_saved_groupings_widget(f\"{dataset_path}/clustering\", spine_dataset, spine_metrics,\n",
    "                                       chord, classic, manual_classification))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}